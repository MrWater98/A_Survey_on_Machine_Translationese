\section{Introduction}

The field of machine translation (MT) has undergone a dramatic evolution, advancing from rule-based systems to the sophisticated Large Language Models (LLMs) of today. This progression, marked by milestones such as phrase-based Statistical MT (SMT) \cite{koehn2003statistical}, the attention mechanism in Neural MT (NMT) \cite{bahdanau2015neural}, and the Transformer architecture \cite{vaswani2017attention}, has significantly enhanced translation quality and accessibility. However, a persistent byproduct of this technological advancement is ``translationese''---systematic linguistic artifacts that distinguish translated text from text originally authored in the target language \cite{gellerstam1986translationese}.

Translationese is not a monolithic phenomenon; its characteristics have co-evolved with MT paradigms, as illustrated in Figure~\ref{fig:timeline}. Early corpus-based studies, building on foundational concepts like translation universals \cite{baker1993corpus}, identified general features such as simplification and normalization in human-translated texts \cite{laviosa1998corpus}. The SMT era brought a focus on source-language interference, with researchers demonstrating that ``source language markers'' could be used to identify the original language of a translation with high accuracy \cite{van2008source,koppel2011translationese}. The advent of NMT, while reducing overt grammatical errors, introduced more subtle biases. These include a loss of lexical richness \cite{vanmassenhove2019getting} and the emergence of ``post-editese,'' a hybrid artifact found in human-edited NMT outputs \cite{toral2019post}. Most recently, LLMs have been shown to produce their own distinct form of translationese, often characterized by excessive literalism and a tendency to ``over-normalize'' text \cite{li2025how,wang2024benchmarking}.

In response to this evolving challenge, a substantial body of research has developed methods to monitor, document, and mitigate the effects of translationese. The task of detecting translationese has progressed from manual analysis and feature-based classifiers \cite{baroni2006new,volansky2015features} to highly accurate neural models \cite{pylypenko2021comparing,amponsah2022explaining}. Characterizing the specific features of translationese across different MT architectures has also been a key focus \cite{riley2020translationese,zhou2024comparison}. Furthermore, various mitigation strategies have been proposed, ranging from data filtering and model fine-tuning \cite{lembersky2013improving,jourdan2025translationese} to leveraging translationese as a form of synthetic data to aid low-resource languages \cite{dabre2024pretraining}. The proliferation of machine-generated text also raises long-term concerns, such as ``model collapse,'' where models trained on synthetic data experience a decline in performance \cite{shumailov2024ai}.

While several surveys have covered specific aspects of MT or translation studies, a comprehensive overview that chronologically links the evolution of MT technology with the study of its primary artifact, translationese, is currently lacking. This paper aims to fill that gap. We provide a systematic survey of translationese research, structured around the major technological paradigms: corpus-based studies, SMT, NMT, and LLMs. For each era, we review seminal works, key findings, and the development of methods for detection, characterization, and mitigation. By presenting this integrated historical perspective, we aim to provide a clear framework for understanding the current state of the field and to highlight the critical challenges and opportunities for future research in an era of ubiquitous MT.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{mt_translationese_timeline.png}
    \caption{The evolution of Machine Translation (MT) systems and the co-evolving study of translationese. The timeline charts the progression of MT paradigms, the key characteristics of translationese identified in each era, and the development of methods for its detection.}
    \label{fig:timeline}
\end{figure*}
