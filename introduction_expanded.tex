\section{Introduction}

The field of machine translation (MT) has undergone a dramatic evolution, advancing from rule-based systems to the sophisticated Large Language Models (LLMs) of today. This progression, marked by milestones such as phrase-based Statistical MT (SMT) \cite{koehn2003statistical}, the attention mechanism in Neural MT (NMT) \cite{bahdanau2015neural}, and the Transformer architecture \cite{vaswani2017attention}, has significantly enhanced translation quality and accessibility. However, a persistent byproduct of this technological advancement is ``translationese''---systematic linguistic artifacts that distinguish translated text from text originally authored in the target language \cite{gellerstam1986translationese}. Understanding the nature and evolution of translationese is not merely an academic exercise; it is critical for improving MT systems, ensuring fairness, and mitigating long-term risks such as the degradation of training data.

Translationese is not a monolithic phenomenon; its characteristics have co-evolved with MT paradigms. Early corpus-based studies, building on foundational concepts like translation universals \cite{baker1993corpus}, identified general features such as simplification (reduced lexical diversity) and normalization (adherence to typical target language patterns) in human-translated texts \cite{laviosa1998corpus}. The SMT era brought a focus on source-language interference, with researchers demonstrating that ``source language markers''---subtle linguistic patterns influenced by the source text---could be used to identify the original language of a translation with high accuracy \cite{van2008source,koppel2011translationese}. This discovery highlighted that even statistically optimized systems carry traces of their source material.

The advent of NMT, while reducing overt grammatical errors, introduced more subtle biases. These include a continued loss of lexical richness \cite{vanmassenhove2019getting}, a tendency towards more conservative and less creative language use, and the emergence of ``post-editese,'' a hybrid artifact found in human-edited NMT outputs that retains some machine-like qualities \cite{toral2019post}. Most recently, LLMs have been shown to produce their own distinct form of translationese. While capable of highly fluent output, they often exhibit excessive literalism, a phenomenon where the model adheres too closely to the source text's structure, and a strong tendency to ``over-normalize'' text, smoothing out stylistic variations \cite{li2025how,wang2024benchmarking}. These subtle artifacts can be difficult for human evaluators to detect but can have significant downstream effects.

In response to this evolving challenge, a substantial body of research has developed methods to monitor, document, and mitigate the effects of translationese. The task of detecting translationese has progressed from manual analysis and feature-based classifiers, which relied on hand-crafted linguistic features \cite{baroni2006new,volansky2015features}, to highly accurate neural models that can learn relevant features automatically \cite{pylypenko2021comparing,amponsah2022explaining}. Characterizing the specific features of translationese across different MT architectures has also been a key focus, with studies comparing the outputs of SMT, NMT, and LLM-based systems to pinpoint their unique linguistic fingerprints \cite{riley2020translationese,zhou2024comparison}. Furthermore, various mitigation strategies have been proposed. These range from data-centric approaches, such as filtering training data to remove translationese \cite{lembersky2013improving}, to model-centric techniques, like fine-tuning models to better emulate human-written text \cite{jourdan2025translationese}. Some work has even explored leveraging translationese as a form of synthetic data to aid low-resource languages \cite{dabre2024pretraining}. The proliferation of machine-generated text also raises long-term concerns, such as ``model collapse,'' where models trained on synthetic data experience a decline in performance over time \cite{shumailov2024ai}.

While several surveys have covered specific aspects of MT or translation studies, a comprehensive overview that chronologically links the evolution of MT technology with the study of its primary artifact, translationese, is currently lacking. This paper aims to fill that gap. We provide a systematic survey of translationese research, structured around the major technological paradigms: corpus-based studies, SMT, NMT, and LLMs. For each era, we review seminal works, key findings, and the development of methods for detection, characterization, and mitigation. By presenting this integrated historical perspective, we aim to provide a clear framework for understanding the current state of the field and to highlight the critical challenges and opportunities for future research in an era of ubiquitous MT. Our survey is intended to be a valuable resource for both newcomers seeking a structured introduction to the field and for experienced researchers looking to situate their work within a broader historical context.
